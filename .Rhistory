install.packages("tidyverse", repos = "http://cran.us.r-project.org")
install.packages("tidytext", repos = "http://cran.us.r-project.org")
install.packages("tidyverse", repos = "http://cran.us.r-project.org")
install.packages("tidytext", repos = "http://cran.us.r-project.org")
install.packages("gutenbergr", repos = "http://cran.us.r-project.org")
install.packages("jsonlite", repos = "http://cran.us.r-project.org")
install.packages("httr", repos = "http://cran.us.r-project.org")
install.packages("jsonlite", repos = "http://cran.us.r-project.org")
install.packages("gutenbergr", repos = "http://cran.us.r-project.org")
install.packages("kableExtra", repos = "http://cran.us.r-project.org")
library(gutenbergr)
library(tidytext)
library(dplyr)
library(wordcloud)
library(sentimentr)
library(stringr)
library(knitr)
gutenberg_works(author == "Dickinson, Emily")
emily_poems <- gutenberg_download(12242)
install.packages("kableExtra", repos = "http://cran.us.r-project.org")
install.packages("gutenbergr", repos = "http://cran.us.r-project.org")
tidy_emily <- emily_poems %>%
unnest_tokens(word, text)
# make one word per row
bing_word_counts <- tidy_emily %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
# taken from the text
tidy_emily %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 50))
# word cloud
dickinson <- gutenberg_download(12242)
install.packages("gutenbergr", repos = "http://cran.us.r-project.org")
text_combined <- paste(dickinson$text, collapse = " ")
#make into one long text
sentences <- get_sentences(text_combined)
#split into sentences using punctuation rules
flat_sentences <- unlist(sentences)
#make into vector
sentiment_scores <- sentiment(flat_sentences)
results <- cbind(sentiment_scores, sentence = flat_sentences)
#combine the score and sentences
overall_sentiment <- mean(results$sentiment)
if (overall_sentiment > 0) {
cat("Overall sentiment is positive.\n")
} else if (overall_sentiment < 0) {
cat("Overall sentiment is negative.\n")
} else {
cat("Overall sentiment is neutral.\n")
}
# Get the overall sentiment score
print(overall_sentiment)
top_pos <- results[order(results$sentiment, decreasing = TRUE), ][1:5, ]
top_neg <- results[order(results$sentiment, decreasing = FALSE), ][1:5, ]
# Top Positive
kable(top_pos, format = "latex", booktabs = TRUE)
library(httr)
library(jsonlite)
headers <- add_headers(
`Content-Type` = "application/json",
`Ocp-Apim-Subscription-Key` = "CQX3sqKsY2IDsIm9LWN17UFzSwKuFsdoxvnzATf0iRl3KfKqnwFXJQQJ99BDACYeBjFXJ3w3AAAaACOGotfN"
)
data <- '{"documents": [{"id": "2", "text": "
If I can stop one heart from breaking,
I shall not live in vain;
If I can ease one life the aching,
Or cool one pain,
Or help one fainting robin
Unto his nest again,
I shall not live in vain."}]}'
res <- POST(
url = "https://week10data607.cognitiveservices.azure.com/text/analytics/v3.1/sentiment?opinionMining=false",
headers,
body = data,
encode = "raw"
)
# Extract sentiment
parsed <- content(res, as = "parsed")
sentiment <- parsed$documents[[1]]$sentiment
print(sentiment)
Sys.getenv()
api_key <- Sys.getenv("CQX3sqKsY2IDsIm9LWN17UFzSwKuFsdoxvnzATf0iRl3KfKqnwFXJQQJ99BDACYeBjFXJ3w3AAAaACOGotfN")
library(httr)
library(jsonlite)
headers <- add_headers(
`Content-Type` = "application/json",
`Ocp-Apim-Subscription-Key` = "api_key"
)
data <- '{"documents": [{"id": "2", "text": "
If I can stop one heart from breaking,
I shall not live in vain;
If I can ease one life the aching,
Or cool one pain,
Or help one fainting robin
Unto his nest again,
I shall not live in vain."}]}'
res <- POST(
url = "https://week10data607.cognitiveservices.azure.com/text/analytics/v3.1/sentiment?opinionMining=false",
headers,
body = data,
encode = "raw"
)
# Extract sentiment
parsed <- content(res, as = "parsed")
sentiment <- parsed$documents[[1]]$sentiment
print(sentiment)
Sys.getenv("MY_API_KEY")  # should return the key (in practice, donâ€™t print it!)
api_key= CQX3sqKsY2IDsIm9LWN17UFzSwKuFsdoxvnzATf0iRl3KfKqnwFXJQQJ99BDACYeBjFXJ3w3AAAaACOGotfN
library(httr)
library(jsonlite)
headers <- add_headers(
`Content-Type` = "application/json",
`Ocp-Apim-Subscription-Key` = Sys.getenv("MY_API_KEY")
)
data <- '{"documents": [{"id": "2", "text": "
If I can stop one heart from breaking,
I shall not live in vain;
If I can ease one life the aching,
Or cool one pain,
Or help one fainting robin
Unto his nest again,
I shall not live in vain."}]}'
res <- POST(
url = "https://week10data607.cognitiveservices.azure.com/text/analytics/v3.1/sentiment?opinionMining=false",
headers,
body = data,
encode = "raw"
)
# Extract sentiment
parsed <- content(res, as = "parsed")
sentiment <- parsed$documents[[1]]$sentiment
print(sentiment)
library(httr)
library(jsonlite)
headers <- add_headers(
`Content-Type` = "application/json",
`Ocp-Apim-Subscription-Key` = Sys.getenv("api_key")
)
data <- '{"documents": [{"id": "2", "text": "
If I can stop one heart from breaking,
I shall not live in vain;
If I can ease one life the aching,
Or cool one pain,
Or help one fainting robin
Unto his nest again,
I shall not live in vain."}]}'
res <- POST(
url = "https://week10data607.cognitiveservices.azure.com/text/analytics/v3.1/sentiment?opinionMining=false",
headers,
body = data,
encode = "raw"
)
# Extract sentiment
parsed <- content(res, as = "parsed")
sentiment <- parsed$documents[[1]]$sentiment
print(sentiment)
library(httr)
library(jsonlite)
headers <- add_headers(
`Content-Type` = "application/json",
`Ocp-Apim-Subscription-Key` <- Sys.getenv("api_key")
)
data <- '{"documents": [{"id": "2", "text": "
If I can stop one heart from breaking,
I shall not live in vain;
If I can ease one life the aching,
Or cool one pain,
Or help one fainting robin
Unto his nest again,
I shall not live in vain."}]}'
res <- POST(
url = "https://week10data607.cognitiveservices.azure.com/text/analytics/v3.1/sentiment?opinionMining=false",
headers,
body = data,
encode = "raw"
)
# Extract sentiment
parsed <- content(res, as = "parsed")
sentiment <- parsed$documents[[1]]$sentiment
print(sentiment)
View(json_data)
View(grades)
View(headers)
MY_API_KEY=CQX3sqKsY2IDsIm9LWN17UFzSwKuFsdoxvnzATf0iRl3KfKqnwFXJQQJ99BDACYeBjFXJ3w3AAAaACOGotfN
file.edit("~/.Renviron")
file.edit("~/.Renviron")
library(httr)
library(jsonlite)
headers <- add_headers(
`Content-Type` = "application/json",
`Ocp-Apim-Subscription-Key` <- Sys.getenv("api_key")
)
data <- '{"documents": [{"id": "2", "text": "
If I can stop one heart from breaking,
I shall not live in vain;
If I can ease one life the aching,
Or cool one pain,
Or help one fainting robin
Unto his nest again,
I shall not live in vain."}]}'
res <- POST(
url = "https://week10data607.cognitiveservices.azure.com/text/analytics/v3.1/sentiment?opinionMining=false",
headers,
body = data,
encode = "raw"
)
# Extract sentiment
parsed <- content(res, as = "parsed")
sentiment <- parsed$documents[[1]]$sentiment
print(sentiment)
Sys.getenv("MY_API_KEY")
Sys.getenv("api_key")
library(httr)
library(jsonlite)
headers <- add_headers(
`Content-Type` = "application/json",
`Ocp-Apim-Subscription-Key` <- Sys.getenv("api_key")
)
data <- '{"documents": [{"id": "2", "text": "
If I can stop one heart from breaking,
I shall not live in vain;
If I can ease one life the aching,
Or cool one pain,
Or help one fainting robin
Unto his nest again,
I shall not live in vain."}]}'
res <- POST(
url = "https://week10data607.cognitiveservices.azure.com/text/analytics/v3.1/sentiment?opinionMining=false",
headers,
body = data,
encode = "raw"
)
# Extract sentiment
parsed <- content(res, as = "parsed")
sentiment <- parsed$documents[[1]]$sentiment
print(sentiment)
library(httr)
library(jsonlite)
headers <- add_headers(
`Content-Type` = "application/json",
`Ocp-Apim-Subscription-Key` = Sys.getenv("api_key")
)
data <- '{"documents": [{"id": "2", "text": "
If I can stop one heart from breaking,
I shall not live in vain;
If I can ease one life the aching,
Or cool one pain,
Or help one fainting robin
Unto his nest again,
I shall not live in vain."}]}'
res <- POST(
url = "https://week10data607.cognitiveservices.azure.com/text/analytics/v3.1/sentiment?opinionMining=false",
headers,
body = data,
encode = "raw"
)
# Extract sentiment
parsed <- content(res, as = "parsed")
sentiment <- parsed$documents[[1]]$sentiment
print(sentiment)
library(gutenbergr)
library(tidytext)
library(dplyr)
library(wordcloud)
library(sentimentr)
library(stringr)
library(knitr)
gutenberg_works(author == "Dickinson, Emily")
emily_poems <- gutenberg_download(12242)
tidy_emily <- emily_poems %>%
unnest_tokens(word, text)
# make one word per row
bing_word_counts <- tidy_emily %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
# taken from the text
print(bing_word_counts)
tidy_emily %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 50))
# word cloud
library(gutenbergr)
library(tidytext)
library(dplyr)
library(wordcloud)
library(sentimentr)
library(stringr)
library(knitr)
gutenberg_works(author == "Dickinson, Emily")
emily_poems <- gutenberg_download(12242)
tidy_emily <- emily_poems %>%
unnest_tokens(word, text)
# make one word per row
bing_word_counts <- tidy_emily %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
# taken from the text
print(sum(bing_word_counts))
library(gutenbergr)
library(tidytext)
library(dplyr)
library(wordcloud)
library(sentimentr)
library(stringr)
library(knitr)
gutenberg_works(author == "Dickinson, Emily")
emily_poems <- gutenberg_download(12242)
tidy_emily <- emily_poems %>%
unnest_tokens(word, text)
# make one word per row
bing_word_counts <- tidy_emily %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
# taken from the text
print(bing_word_counts)
tidy_emily %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 50))
# word cloud
library(gutenbergr)
library(tidytext)
library(dplyr)
library(wordcloud)
library(sentimentr)
library(stringr)
library(knitr)
gutenberg_works(author == "Dickinson, Emily")
emily_poems <- gutenberg_download(12242)
tidy_emily <- emily_poems %>%
unnest_tokens(word, text)
# make one word per row
bing_word_counts <- tidy_emily %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment, sort = TRUE) %>%
ungroup()
# taken from the text
print(bing_word_counts)
tidy_emily %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 50))
# word cloud
install.packages("tidyverse", repos = "http://cran.us.r-project.org")
install.packages("tidytext", repos = "http://cran.us.r-project.org")
install.packages("gutenbergr", repos = "http://cran.us.r-project.org")
install.packages("jsonlite", repos = "http://cran.us.r-project.org")
install.packages("httr", repos = "http://cran.us.r-project.org")
install.packages("kableExtra", repos = "http://cran.us.r-project.org")
library(gutenbergr)
library(tidytext)
library(dplyr)
library(wordcloud)
library(sentimentr)
library(stringr)
library(knitr)
gutenberg_works(author == "Dickinson, Emily")
emily_poems <- gutenberg_download(12242)
tidy_emily <- emily_poems %>%
unnest_tokens(word, text)
# make one word per row
bing_word_counts <- tidy_emily %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment, sort = TRUE) %>%
ungroup()
# taken from the text
print(bing_word_counts)
tidy_emily %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 50))
# word cloud
install.packages("tidytext", repos = "http://cran.us.r-project.org")
install.packages("gutenbergr", repos = "http://cran.us.r-project.org")
install.packages("jsonlite", repos = "http://cran.us.r-project.org")
install.packages("httr", repos = "http://cran.us.r-project.org")
install.packages("kableExtra", repos = "http://cran.us.r-project.org")
install.packages("jsonlite", repos = "http://cran.us.r-project.org")
install.packages("httr", repos = "http://cran.us.r-project.org")
install.packages("kableExtra", repos = "http://cran.us.r-project.org")
install.packages("tidyverse", repos = "http://cran.us.r-project.org")
dickinson <- gutenberg_download(12242)
text_combined <- paste(dickinson$text, collapse = " ")
#make into one long text
sentences <- get_sentences(text_combined)
#split into sentences using punctuation rules
flat_sentences <- unlist(sentences)
#make into vector
sentiment_scores <- sentiment(flat_sentences)
results <- cbind(sentiment_scores, sentence = flat_sentences)
#combine the score and sentences
overall_sentiment <- mean(results$sentiment)
if (overall_sentiment > 0) {
cat("Overall sentiment is positive.\n")
} else if (overall_sentiment < 0) {
cat("Overall sentiment is negative.\n")
} else {
cat("Overall sentiment is neutral.\n")
}
# Get the overall sentiment score
print(overall_sentiment)
top_pos <- results[order(results$sentiment, decreasing = TRUE), ][1:5, ]
top_neg <- results[order(results$sentiment, decreasing = FALSE), ][1:5, ]
kable(top_pos, format = "latex")
kable(top_neg, format = "latex")
library(httr)
library(jsonlite)
headers <- add_headers(
`Content-Type` = "application/json",
`Ocp-Apim-Subscription-Key` = Sys.getenv("api_key")
)
data <- '{"documents": [{"id": "2", "text": "
If I can stop one heart from breaking,
I shall not live in vain;
If I can ease one life the aching,
Or cool one pain,
Or help one fainting robin
Unto his nest again,
I shall not live in vain."}]}'
res <- POST(
url = "https://week10data607.cognitiveservices.azure.com/text/analytics/v3.1/sentiment?opinionMining=false",
headers,
body = data,
encode = "raw"
)
# Extract sentiment
parsed <- content(res, as = "parsed")
sentiment <- parsed$documents[[1]]$sentiment
print(sentiment)
install.packages("jsonlite", repos = "http://cran.us.r-project.org")
install.packages("httr", repos = "http://cran.us.r-project.org")
install.packages("kableExtra", repos = "http://cran.us.r-project.org")
install.packages("tidyverse", repos = "http://cran.us.r-project.org")
install.packages("kableExtra", repos = "http://cran.us.r-project.org")
install.packages("httr", repos = "http://cran.us.r-project.org")
install.packages("tidyverse", repos = "http://cran.us.r-project.org")
install.packages("kableExtra", repos = "http://cran.us.r-project.org")
install.packages("httr", repos = "http://cran.us.r-project.org")
install.packages("kableExtra", repos = "http://cran.us.r-project.org")
install.packages("kableExtra", repos = "http://cran.us.r-project.org")
install.packages("httr", repos = "http://cran.us.r-project.org")
library(httr)
library(jsonlite)
headers <- add_headers(
`Content-Type` = "application/json",
`Ocp-Apim-Subscription-Key` = Sys.getenv("api_key")
)
data <- '{"documents": [{"id": "2", "text": "
If I can stop one heart from breaking,
I shall not live in vain;
If I can ease one life the aching,
Or cool one pain,
Or help one fainting robin
Unto his nest again,
I shall not live in vain."}]}'
res <- POST(
url = "https://week10data607.cognitiveservices.azure.com/text/analytics/v3.1/sentiment?opinionMining=false",
headers,
body = data,
encode = "raw"
)
res
library(httr)
library(jsonlite)
headers <- add_headers(
`Content-Type` = "application/json",
`Ocp-Apim-Subscription-Key` = Sys.getenv("api_key")
)
data <- '{"documents": [{"id": "2", "text": "
If I can stop one heart from breaking,
I shall not live in vain;
If I can ease one life the aching,
Or cool one pain,
Or help one fainting robin
Unto his nest again,
I shall not live in vain."}]}'
res <- POST(
url = "https://week10data607.cognitiveservices.azure.com/text/analytics/v3.1/sentiment?opinionMining=false",
headers,
body = data,
encode = "raw"
)
content(res)$documents[1]$sentiment
library(httr)
library(jsonlite)
headers <- add_headers(
`Content-Type` = "application/json",
`Ocp-Apim-Subscription-Key` = Sys.getenv("api_key")
)
data <- '{"documents": [{"id": "2", "text": "
If I can stop one heart from breaking,
I shall not live in vain;
If I can ease one life the aching,
Or cool one pain,
Or help one fainting robin
Unto his nest again,
I shall not live in vain."}]}'
res <- POST(
url = "https://week10data607.cognitiveservices.azure.com/text/analytics/v3.1/sentiment?opinionMining=false",
headers,
body = data,
encode = "raw"
)
res
content(res)$documents[[1]]$sentiment
